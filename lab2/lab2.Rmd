---
title: "Laboratório 2 - Métodos Baseados em Árvores e Florestas Aleatórias"
author: "ME905"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages
library(rpart)
library(ggplot2)
library(dplyr)
```



# 1. Leitura dos Dados
```{r}
# lendo dados
library(readr)
mnist <- read_csv("MNIST0178.csv")
mnist$y <- as.factor(mnist$y)
```



# 2. Visualização de Dígitos

## a)
```{r}
# visualizando dados
converte_df <- function(vetor_covariaveis) {
  vetor_covariaveis <- as.vector(unlist(vetor_covariaveis))
  if(length(vetor_covariaveis) != 784){
    stop("Passe um vetor com 784 valores!")
  }
  
  pos_x <- rep(1:28, each = 28)
  pos_y <- rep(1:28, times = 28)
  data.frame(pos_x, pos_y, valor = vetor_covariaveis)
}

visnum <- function(df) {
  df %>% ggplot(aes(x = pos_y, y = pos_x, fill = valor)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "black") +
    theme_void() +
    scale_y_reverse() +
    theme(legend.position = "none")
}

# lendo números
n1 <- converte_df(mnist[1,-1])
n2 <- converte_df(mnist[3,-1])
n3 <- converte_df(mnist[6,-1])
n4 <- converte_df(mnist[7,-1])

visnum(n1)  # 0
visnum(n2)  # 1
visnum(n3)  # 7
visnum(n4)  # 8
```

Vemos os números 0, 1, 7 e 8, respectivamente.

## b)
Possívelmente, os números mais difíceis de serem distinguidos são os dígitos 1 e 7 pela sua semelhança e,analogamente, teremos menos dificuldade entre o 0 e o 8 com relação aos demais.



# 3. Árvore de Classificação com rpart

```{r}
fit <- rpart(y ~ ., data = mnist)
pred <- predict(fit, type = 'class')

table('Predição' = pred, 'Valores Verdadeiros' = mnist$y)
sum(pred == mnist$y)/length(mnist$y)
```

Houve uma acurácia de aproximadamente 90%, então a árvore foi eficaz. No entanto, nossa conjectura não se provou verdadeira, veja que o 1 e o 7 foram mais confundidos com o 8 e o 8 foi mais confundido com o 1. Contudo, como nossas suspeitas, o 0 foi mais confundido com o 8.



# 4. Florestas Aleatórias (com Estratégias Manuais)

```{r}
# seleciona colunas do banco de dados ------------------------------------------
# data banco de dados com a primeira coluna sendo a resposta.
# p número de variáveis selecionadas. Se não especificada, será usado a raíz
# quadrada da quantidade de preditoras no banco de dados.

select_cols <- function(data, p = NULL) {
  if (is.null(p)) p <- round(sqrt(ncol(data)))
  p_tot <- ncol(data)                     # total de variáveis
  select_var <- c(1, sample(2:p_tot, p))  # variáveis selecionadas
  return(data[select_var])
}

# bootstrap --------------------------------------------------------------------
# data banco de dados com a primeira coluna sendo a resposta.
bs <- function(data) {
  return(sample(1:nrow(data), nrow(data), replace = T))
}

# gera floresta aleatória ------------------------------------------------------
# data banco de dados com a primeira coluna sendo a resposta.
# n_tree quantidade de árvores de decisão da floresta.
random_forest <- function(data, n_tree, ...) {
  controle <- rpart.control(...)
  
  # criando objetos
  floresta <- vector(mode = 'list', length = n_tree)
  OOBE <- numeric(n_tree)
  
  for (i in 1:n_tree) {
    linhas_sorteadas <- bs(data)
    db_bs <- (data |> select_cols())[linhas_sorteadas,]
    
    floresta[[i]] <- rpart(y ~ ., db_bs, control = controle)
    OOBE[i] <- 1 - mean(predict(floresta[[i]], type = 'class',
                                newdata = data[-unique(linhas_sorteadas),]) ==
                          data[-unique(linhas_sorteadas),]$y)
  }
  return(list('forest' = floresta, 'acuracia' = 1 - OOBE))
}

forest <- random_forest(mnist, n_tree=20)
```

```{r}
# teste keik
random_forest <- function(data, n_tree, ...) {
  controle <- rpart.control(...)  # controle de parâmetros do rpart
  
  # definindo objetos
  n <- nrow(data)  # número de observações
  floresta <- vector(mode = 'list', length = n_tree)  # armazenar as árvores
  p <- round(sqrt(ncol(data)))    # sugestão apresentada em aula e livro
  oob_preds <- vector('list', n)  # predições OOB por observação
  oob_error_curve <- numeric(n_tree)  # curva do erro OOB ao longo das árvores
  classes <- c('0', '1', '7', '8')
  
  # matriz de NA para armazenar erros OOB por classe NA para se proteger de
  # alguma árvore não possuir nenhum dado de uma classe específica,
  # impossibilitando calcular seu erro (muito dificil de ocorrer na prática,
  # não?!)
  oob_class_errors <- matrix(NA, nrow = n_tree, ncol = length(classes))
  colnames(oob_class_errors) <- classes
  
  for (t in 1:n_tree) {
    linhas_sorteadas <- bs(data)
    oob_indices <- setdiff(1:n, unique(linhas_sorteadas)) # linhas não sorteadas
    
    p_tot <- ncol(data)
    selected_cols <- c(1, sample(2:p_tot, p))  # y e preditoras selecionadas
    db_bs <- data[linhas_sorteadas, selected_cols]
    
    floresta[[t]] <- rpart(y ~ ., db_bs, control = controle)  # ajustando arvore
    
    oob_data <- data[oob_indices, selected_cols]  # obs não usadas na arvore
    pred <- predict(floresta[[t]], newdata = oob_data, type = 'class')
    # for (j in seq_along(oob_indices)) {
    #   idx <- oob_indices[j]
    #   oob_preds[[idx]] <- c(
    #     #oob_preds[[idx]],
    #     as.character(pred[j]))
    # }
    # verificar ideia de oob preds, mas acho que pode ser substituido por
    oob_preds[oob_indices] <- as.character(pred)
    
    # continuar verificação daqui
    # atualiza predições OOB
    oob_final_pred <- character(n)
    for (i in 1:n) {
      if (length(oob_preds[[i]]) > 0) {
        oob_final_pred[i] <- names(sort(table(oob_preds[[i]]), decreasing = TRUE))[1]
      } else {
        oob_final_pred[i] <- NA
      }
    }
    
    real <- as.character(data$y)
    valido <- !is.na(oob_final_pred)
    oob_error_curve[t] <- mean(oob_final_pred[valido] != real[valido])
    
    # erro por classe
    for (j in seq_along(classes)) {
      classe <- classes[j]
      indices_classe <- which(real == classe & valido)
      if (length(indices_classe) > 0) {
        oob_class_errors[t, j] <- mean(oob_final_pred[indices_classe] != real[indices_classe])
      } else {
        oob_class_errors[t, j] <- NA
      }
    }
  }
  
  return(list(
    forest = floresta,
    oob_error_curve = oob_error_curve,
    oob_class_errors = oob_class_errors,
    oob_accuracy = 1 - oob_error_curve[n_tree]
  ))
}


modelo <- random_forest(mnist, n_tree = 10, minbucket = 10, maxdepth = 30, cp = 0.0001)

oob.error.data <- data.frame(
  Trees = rep(1:10, times = 5),
  Type = factor(rep(c("OOB", colnames(modelo$oob_class_errors)), each = 10),
                levels = c("OOB", colnames(modelo$oob_class_errors))),
  Error = c(modelo$oob_error_curve,
            as.vector(modelo$oob_class_errors))
)

ggplot(data = oob.error.data, aes(x = Trees, y = Error)) +
  geom_line(aes(color = Type), linewidth = 1.2) +
  labs(title = "Erro OOB e de classificação por número de árvores",
       x = "Número de árvores",
       y = "Erro",
       color = "Tipo") +
  theme_minimal(base_size = 14)
```



# 5. Análise dos Erros

```{r}
# função voto_da_maioria -------------------------------------------------------
# forest precisa ser um objeto da saída da função `random_forest`.
# data banco de dados para predição.

voto_da_maioria <- function(forest, data) {
  pred_tree <- vector(mode='list', length(forest$forest))
  
  for (i in 1:length(forest$forest)) {
    pred_tree[[i]] <- unname(predict(forest$forest[[i]], type = 'class',
                                     newdata = data))
  }
  voto <- data.frame(tree = unname(do.call(cbind.data.frame, pred_tree)))
  apply(voto, 1, \(x) names(which.max(table(unlist(x)))))
}

# fazendo predição e modificando `converte_df` ---------------------------------
pred_forest <- voto_da_maioria(forest, mnist)
head(pred_forest)  # predição para 6 primeiras obs


mnist_com_pred <- cbind('pred' = pred_forest, mnist)


combinacoes_erro <- data.frame()

for (y in c(0,1,7,8)) {
  for (pred in c(0,1,7,8)) {
    combinacoes_erro <- bind_rows(combinacoes_erro, data.frame(pred = pred,
        mnist[mnist_com_pred$y == y & mnist_com_pred$pred == pred,][1,])
    )
  }
}


# modificando converte_df 
converte_df_mod <- function(data) {
  pos_x <- rep(1:28, each = 28)
  pos_y <- rep(1:28, times = 28)
  resultado <- data.frame()
  
  for (i in 1:16) {
    pred <- data$pred[i]
    y <- data$y[i]
    vetor_covariaveis <- as.vector(unlist(data[i,-c(1,2)]))
    resultado <- bind_rows(resultado, bind_cols(pred = pred, y = y,
                    data.frame(pos_x, pos_y, valor = vetor_covariaveis)))
  }
  
  return(resultado)
}

erro_long <- converte_df_mod(combinacoes_erro)

# plotando
visnum(erro_long) +
  facet_grid(rows = vars(pred), cols = vars(y)) +
  theme_bw() +
  labs(x = 'Verdadeiro', y = 'Predito',
       title = 'Situações possíveis') +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        legend.position = "none")
```



# 6. Predição em Novos Dados







