---
title: "Laboratório 3 - Boosting + SVM"
author: "ME905"
output: pdf_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)

set.seed(123123)

# packages
library(rpart)
```

# Instruções

- Baixe os arquivos `MNIST35.csv` e `MNIST35-teste.csv` disponíveis em: https://drive.google.com/drive/folders/1Tcsy6DuRGIaL2Dn34QqsZyeRd7hSo0XZ?usp=sharing.
- Para a implementação do algoritmo de Boosting, utilize **exclusivamente a função `rpart`** do pacote `rpart` para o treinamento das árvores de decisão.
- **Não utilize funções ou pacotes que automatizem o processo de validação cruzada.**
- Além do código, inclua comentários e análises textuais sempre que julgar necessário.

---

## 1. Leitura dos dados

Carregue o conjunto de dados `MNIST35.csv` na variável `mnist`. Esse arquivo contém um `data.frame` com:

- 784 colunas (`x1` a `x784`) representando as covariáveis;
- 1 coluna chamada `y`, indicando o dígito manuscrito (3 ou 5).

Informações adicionais:

- Cada linha representa uma imagem.
- As variáveis `x001` a `x784` indicam intensidades em escala de cinza, com valores entre 0 e 255 — quanto maior o valor, mais escuro o pixel.
- Cada variável representa uma posição em um grid 28x28. Por exemplo, `x2` representa a posição (1,2), enquanto `x30` representa a posição (2,2).

```{r, warning=F, message=F}
library(readr)
mnist <- read_csv("MNIST35.csv")

# recategorizando 3 em 0 e 5 em 1
mnist$y <- ifelse(mnist$y == 3, 0, 1)
```

---

## 2. Boosting para respostas binárias

Considere o algoritmo de Boosting estudado em aula (Capítulo 8 do livro *Introduction to Statistical Learning*), com a variável resposta $y$ assumindo valores 0 (dígito 3) ou 1 (dígito 5).

Adote como preditor inicial (modelo nulo) o valor constante $f_0(x) = 0{,}5$, ou seja, todos os dígitos são inicialmente classificados com probabilidade 0,5 de corresponder ao dígito 5.

Interprete os resíduos obtidos a partir desse modelo inicial.

Teremos dois casos, o primeiro será quando a resposta for $1$, então, nosso resíduo será $1 - 0.5 = 0.5$, o segundo caso ocorre quando nossa resposta é $0$ gerando o resíduo $0 - 0.5 = -0.5$.

---

## 3. Implementação do algoritmo de Boosting

Implemente uma função que realize o algoritmo de Boosting no contexto descrito acima. Sua função deve receber como parâmetros:

- A taxa de aprendizado;
- O número de iterações (ou árvores);
- A profundidade máxima permitida para cada árvore.

A função deve retornar uma lista com os seguintes elementos:

- O conjunto de dados utilizado no treinamento;
- O erro quadrático médio de predição no conjunto de treinamento:  
  $\frac{1}{n} \sum_{i = 1}^n \left( \hat{f}^*(x_i) - y_i \right)^2$;
- O valor da taxa de aprendizado utilizada;
- Uma lista contendo as árvores ajustadas em cada iteração.

Utilize essa função para ajustar um preditor ao conjunto de dados `MNIST35`.

(**Sugestão bônus**): Atribua uma classe à lista retornada utilizando `class(lista) <- "nome_da_classe"` e implemente um método `print.nome_da_classe` apropriado.

```{r}
# função: boosting ----------------
# f0 valor constante do modelo nulo
# lambda taxa de aprendizado
# n_tree número de árvores
# tree_depth profundidade da árvore
# data dados

boosting <- function(f0 = 0.5, lambda, n_tree, tree_depth, data) {
  n <- nrow(data)
  pred <- rep(0, n)
  df <- cbind('y' = data$y - f0, data[-1])
  forest <- list()
  
  # ajustando árvores
  for (i in 1:n_tree) {
    forest[[i]] <- rpart(y ~ . , df, control = rpart.control(maxdepth = tree_depth))
    df$y <- df$y - lambda * predict(forest[[i]])  # atualizando resíduo
  }
  
  # predição final
  f_hat <- cbind(f0, vapply(forest, predict, numeric(n)) * lambda)
  EQM <- mean((rowSums(f_hat) - data$y)^2)
  
  result <- list(data = data, EQM = EQM, lambda = lambda, forest = forest, f0 = f0)
  class(result) <- 'boosting'
  return(result)
}

# função: print.boosting ----------
print.boosting <- function(modelo) {
  cat('EQM:', round(modelo$EQM, 4),
      '\nlambda:', modelo$lambda)
}
```

```{r, eval = F, echo = F}
# testando função
boost <- boosting(0.5, lambda = 0.5, n_tree = 3, tree_depth = 1, data = mnist)
boost
```


---

## 4. Função de predição

Implemente uma função para gerar predições a partir do resultado da função de Boosting. Essa função deve receber:

- O objeto resultante do algoritmo de Boosting;
- Um número $K$ de árvores a serem utilizadas;
- Um tipo de predição (`"classe"` ou `"prob"`);
- Um `data.frame` com as colunas `x001`, ..., `x784`.

A função deve retornar as predições correspondentes a cada linha do `data.frame`, considerando apenas as primeiras $K$ árvores. No caso do tipo `"classe"`, as predições devem ser transformadas para 0 ou 1, de acordo com a classe mais provável.

Teste sua função em um subconjunto do `MNIST35` com dois valores de $K$ (um pequeno e outro grande) e compare os resultados.

(**Sugestão bônus**): Caso tenha definido uma classe para o objeto de Boosting, implemente a função de predição via o método `predict.nome_da_classe`.

```{r}
# função: predict.boosting --------
# boosting_result objeto da classe `boosting`
# k número de árvores a serem utilizadas, sem contar f0
# type tipo de predição (`"classe"` ou `"prob"`)
# newdata data frame contendo os valores das preditoras
# pred_plot se TRUE retorna o data.frame com as predições, útil para evitar
#   repetições ao utilizar a função plot.boosting.

predict.boosting <- function(boosting_result, k, type = c('classe', 'prob'),
                             newdata, pred_plot = F) {
  if (class(boosting_result) != 'boosting')
    stop('boosting_result apresenta classe errada')
  if (missing(newdata)) data <- boosting_result$data
  else data <- newdata
  if (missing(k)) k <- length(boosting_result$forest)
  type <- match.arg(type)
  if (k == 0) {
    df_pred <- boosting_result$f0
    pred <- boosting_result$f0
  }
  else {
    df_pred <- cbind(boosting_result$f0, boosting_result$lambda *
                vapply(boosting_result$forest[1:k], \(tree)
                       predict(tree, newdata = data), numeric(nrow(data))))
    pred <- rowSums(df_pred)
  }
  if (type == 'classe') pred <- ifelse(pred > 0.5, 1, 0)
  if (isFALSE(pred_plot)) return(pred)
  else return(df_pred)
}
```

```{r, echo=F, eval=F}
# testando função
boost <- boosting(0.5, lambda = 0.5, n_tree = 3, tree_depth = 1, data = mnist)
debug(predict.boosting)
predict(boost)
```

---

## 5. Curva do erro quadrático médio

Considere duas taxas de aprendizado distintas. Para cada uma, calcule o erro quadrático médio de predição no próprio conjunto de treinamento, variando o número de árvores utilizadas.

Construa um gráfico com:

- Eixo X: número de árvores utilizadas;
- Eixo Y: erro quadrático médio;
- Duas linhas, uma para cada taxa de aprendizado.

```{r Q5}
# função: plot.boosting -----------
# boosting_result objeto da classe `boosting`
# col cor da linha
# add_plot se TRUE, o gráfico é adicionado ao anterior

plot.boosting <- function(boosting_result, col, add_plot = F, newdata) {
  if (missing(newdata)) newdata <- boosting_result$data
  n_tree <- length(boosting_result$forest)  # não conta com f0
  EQM <- vector(mode = 'numeric', length = n_tree + 1)
  if (missing(col)) col <- 'black'
  
  df_pred <- predict(boosting_result, type = 'prob', pred_plot = T, newdata=newdata)
  
  for (i in 1:ncol(df_pred)) {
    EQM[i] <- mean((rowSums(df_pred[, 1:i, drop = F]) - boosting_result$data$y)^2)
  }
  
  if (isFALSE(add_plot))
    plot(0:n_tree, EQM, xlab = 'Número de Árvores', type = 'b', col = col)
  else
    lines(0:n_tree, EQM, xlab = 'Número de Árvores', type = 'b', col = col)
}

# ajustando modelos ---------------
boost1 <- boosting(f0 = 0.5, lambda = 0.75, n_tree = 30, tree_depth = 1, data = mnist)
boost2 <- boosting(f0 = 0.5, lambda = 0.25, n_tree = 30, tree_depth = 1, data = mnist)

# plot
plot(boost1)
plot(boost2, col = 'blue', add_plot = T)
legend('topright', title = expression('valor de ' * lambda), bty = 'n', lty=1,
       pch=21, pt.bg = 'white', legend = c(0.75, 0.25), col = c('black', 'blue'))
```


---

## 6. Predição em novos dados

O arquivo `MNIST35-teste.csv` contém observações **sem a variável resposta**.

- Com base na sua implementação do Boosting, ajuste um preditor final utilizando a estratégia que considerar mais adequada para a escolha dos parâmetros.
- Gere as predições (dígito 3 ou 5) para o conjunto `MNIST35-teste.csv`.
- Salve os resultados em um `data.frame` com uma única coluna chamada `predicao`.
- Exporte esse `data.frame` no formato `.csv`.

**Entrega:** Submeta o arquivo `.csv` com as predições no Moodle, juntamente com seu relatório.

<!-- Para diminuir o tempo de processamento, apenas 4 modelos serão investigados, os cenários serão, $\lambda$ baixo ($0.5$) e alto ($0.85$); `n_tree` baixo ($200$) e alto ($300$). -->

```{r Q6}
# hold out 75/25
i_treino <- sample(1:nrow(mnist), size = nrow(mnist)*0.75, replace = F)
sample_treino <- mnist[i_treino,]
sample_teste <- mnist[-i_treino,]

# ajustando melhor modelo
fit1 <- boosting(f0 = 0.5, lambda = 0.5, n_tree = 200, tree_depth = 1, data = sample_treino)
fit2 <- boosting(f0 = 0.5, lambda = 0.5, n_tree = 200, tree_depth = 1, data = sample_treino)
# fit3 <- boosting(f0 = 0.5, lambda = 0.85, n_tree = 200, tree_depth = 1, data = sample_treino)
# fit4 <- boosting(f0 = 0.5, lambda = 0.85, n_tree = 300, tree_depth = 1, data = sample_treino)

plot(fit2)
plot(fit1, col = 'blue', add_plot = T)
# plot(fit3, col = 'red', add_plot = T)
# plot(fit4, col = 'darkgreen', add_plot = T)
# legend('topright', legend=c('modelo 1', 'modelo 2', 'modelo 3', 'modelo 4'),
#        col='blue', 'black', 'red', 'darkgreen', bty='n')
```

Com base nos gráficos

```{r Q6}
# table(predicao, mnist$y)  # matriz de confusão

# predição
# mnist_teste <- read_csv('MNIST35-teste.csv')
# pred <- predict(fit, type = 'classe', newdata = mnist_teste)
# result <- ifelse(pred == 0, 3, 5)
# write_csv(pred, 'l3-pred-J.csv')
```

